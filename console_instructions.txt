Running from command line
--------------------------
In order to run this code, you will need java and java runtime environment installed (https://www.java.com/en/download/help/download_options.xml), and your computer will have to know where to find java (potentially helpful link: https://www.java.com/en/download/help/path.xml).


+The project currently uses the build tool SBT, which you'll need to install (http://www.scala-sbt.org/download.html)
+To run the project, starting in the main directory, start SBT (command: sbt)
+Compile the main project (command: compile)
+Set project to console (command: project console)
+Compile project (command: compile)
+Run project with arguments, see below for argument options (command: run arg1 arg2 arg3… etc)
+Respond to the prompt that asks you which learner to use

Both EDL and GLA are run using the learn.java program. The first four arguments for each are the same:

GRAMMAR FILE
- should be in the same format as provided TS2000Grammar_secondary.txt. Any lines in the file that don’t match this format will be ignored. This format is based on the format used in Praat. See FileFormats_instructions.txt for more info.

DISTRIBUTION FILE
- should be in the same format as provided TS1_Dist.txt. Any lines in the file that don’t match this format will be ignored. This format is based on the format used in Praat. See FileFormats_instructions.txt for more info.

ITERATIONS
 - this is the number of passes through the data
 - For GLA, reasonable values are around 100-1000 (higher for complex data)
 - The batch EDL learner sees every data point at each iteration, so a reasonable number of iterations is around 100
 - The online EDL learner sees only 1 data point at each iteration, so a comparable number of iterations is 100*the number of data points

FINAL-EVAL_SAMPLE
-How many samples are used to evaluate in the final evaluation
-Default: 1000

 ---------------------------------------------------------------------------------------------------------------------------

EDL

If running EDL, the rest of the arguments are as follows:

LEARNER
1 - batch Expectation Driven Learner in Jarosz (2015). Runs the learning function EDL_batch().
2 - online Expectation Driven Learner in Jarosz (2015). Runs the learning function EDL_online().

GRAMMAR SAMPLE SIZE
- this is the number of times that a grammar is sampled when each pair of constraints is considered during learning
- reasonable values are 50-100

INITIAL BIAS
- If your grammar file encodes which constraints are markedness and which are faithfulness, you can set this to 1 to begin with a stochastically M >> F grammar.
- See FileFormats_instructions.txt for more info.
- set this to 0 by default

To run EDL, use this syntax at the command prompt. You may want to redirect the output to a file or pipe it to less.

run grammar_file dist_file iterations final_eval_sample learn_type gram_sample_size ranking_bias (print args) (maxdepth)

For example: run TS2000Grammar_secondary.txt TS1_Dist.txt 100 1000 1 100 0

You may want to redirect the output to a file or pipe it to less like this:
run TS2000Grammar_secondary.txt TS1_Dist.txt 100 1000 1 100 0 > output.txt
run TS2000Grammar_secondary.txt TS1_Dist.txt 100 1000 1 100 0 | less

----------------------------------------------------------------------------------------------------------------------------

GLA

If running GLA, the rest of the arguments are:

LEARNER
 EIP - Expected Interpretive Parsing in Jarosz (2013)
 RIP - is the original RIP as proposed for Stochastic OT by Boersma (2003)
 RRIP - Resampling RIP in Jarosz (2013)
 randRIP - baseline model which generates a random output as the ‘winner’ when there’s an error
 Baseline - baseline model: generates a brand new ranking/weighting when there’s an error

 GRAMMAR TYPE
 - set this to OT, HG, or ME (maxent)

LEARNING RATE
 - How much constraint ranking/weighting values get bumped when there’s an update
 - typical value is something like 0.1

 NOISE
 - what’s the variance around the ranking/weighting value
 - typical setting is something like 2

 INITIAL BIAS
 - If your grammar file encodes which constraints are markedness and which are faithfulness, you can set this to 1 to begin with an M >> F grammar. 
 - See FileFormats_instructions.txt for more info.
 - set this to 0 by default

 NEGOK?
 - Indicates whether the learner be allowed to use negative weights.
 - Set to 0 to keep weights non-negative (any negative weight is replaced with 0 in updating and in sampling).

To run GLA, use this syntax at the command prompt. As with EDL, you may want to redirect the output to a file or pipe it to less.

run grammar_file dist_file num_samples final_eval_sample learner model learning_rate noise bias NegOK? (print parameters)

For example: run TS2000Grammar_secondary.txt TS1_Dist.txt 1000 1000 EIP OT .1 2 0 0

You may want to redirect the output to a file or pipe it to less like this:
run TS2000Grammar_secondary.txt TS1_Dist.txt 1000 1000 EIP OT .1 2 0 0 > output.txt
run TS2000Grammar_secondary.txt TS1_Dist.txt 1000 1000 EIP OT .1 2 0 0 | less

----------------------------------------------------------------------------------------------------------------------------

Print options

PRINT_INPUT?
- 0 : prints grammar and input
- 1 : doesn't print input
- Default: 0

FINAL-EVAL
- 0 : prints final grammar; accuracy on each output; total error and log likelihood
- 1 : prints final grammar; total error and log likelihood
- Default: 0

MINI-EVAL
- 0: prints grammar; accuracy on each output; total error and log likelihood
- 1: prints grammar; total error and log likelihood
- 2: prints nothing
- Default: 1

MINI-EVAL_FREQ
- How often a mini-evaluation round is performed
- Default: 1
- In order to not perform any intermediate evaluation, set to the same number as iterations

MINI-EVAL_SAMPLE
-How many samples are used to evaluate in a mini-evaluation
-Default: 100

QUIT_EARLY?
-How often the program checks to see if it can quit early
-Quits if the learner has already learned everything
-Default: 100
- In order to not try to quit early, set to the same number as iterations

QUIT_EARLY?_SAMPLE
-How many samples are used to evaluate whether the learner is done learning
-The fewer samples, the faster the program, but the greater the risk of quitting before the learner has really finished learning
-Default: 100

For instance, if you want to run GLA and print as much as possible, run:

run TS2000Grammar_secondary.txt TS1_Dist.txt 1000 1000 EIP OT .1 2 0 0 0 0 0 1 100 100 100

If you want to run GLA as quickly as possible, run:

run TS2000Grammar_secondary.txt TS1_Dist.txt 1000 1000 EIP OT .1 2 0 0 1 1 2 1000 100 1000 100

------------------------------------------------------------------------------------------------------------------------

Advanced options

MAX-DEPTH (EDL only):
-To speed-up run-time we use a tree to keep track of previously generated winners for a given input and ranking prefix. This avoids repeating the optimization process over and over again, but for high numbers of constraints, this can cause the program to run out of memory if the tree is allowed to get too big. This parameter controls the maximum depth (size) of the tree.
-By default, the maximum depth of the tree is set to 8, but you can reduce the depth to reduce the amount of memory used by the program.
-To specify the maximum depth, add the maximum depth you want to the end of the print options.